# -*- coding: utf-8 -*-
"""fake_news.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i9eGTw34ojfoW6rPSuT05AbQKslLeF8V
"""

import pandas as pd
import numpy as np

df_fake = pd.read_csv('news.csv', encoding= 'unicode_escape')
df_fake.head(15)

df_fake = df_fake.drop(['unit_id','source','date','location'],axis=1)
df_fake = df_fake.dropna()

df_fake.head(5)

df_fake = df_fake[0:500]

X =df_fake.iloc[:,:-1].values # independent features -> article_title & article_content
y =df_fake.iloc[:,-1].values  # dependent feature -> label

X[0]

y[0]

from sklearn.feature_extraction.text import CountVectorizer #CountVectorizer is a useful tool for text analysis and is commonly used to convert a collection of text documents into a matrix of token counts.
cv = CountVectorizer(max_features = 1000)
mat_body = cv.fit_transform(X[:,1]).todense() #This will transform the text data in that column into a dense matrix representation using the fit-transform process 'mat_body' <- represents the transformed matrix of token counts.

"""By setting `max_features` to 1000, the `CountVectorizer` will only consider the top 1000 most frequent words or terms in the text data. This helps in limiting the dimensionality of the resulting matrix and can be beneficial in scenarios where you have a large corpus of text and want to focus on the most informative features.

"""

mat_body

cv_head = CountVectorizer(max_features = 1000)
mat_head = cv_head.fit_transform(X[:,0]).todense()

"""The result of the `fit_transform` operation is stored in the variable `mat_head`. This variable represents the transformed matrix of token counts, where each row corresponds to a document (or text sample) and each column represents a specific term or word from the vocabulary.

The `todense()` method is then used to convert the sparse matrix `mat_head` into a dense matrix representation. The dense matrix stores all the elements explicitly, as opposed to a sparse matrix where only non-zero elements are stored explicitly, resulting in a potentially larger memory footprint.

Overall, this code snippet performs the process of tokenizing and counting the occurrences of words in the text data, creating a dense matrix representation for further analysis or modeling tasks.

"""

mat_head

X_mat = np.hstack((mat_head, mat_body))

"""The `np.hstack()` function is used to horizontally stack or concatenate two matrices, `mat_head` and `mat_body`, into a single matrix. This operation combines the columns of the two matrices side by side. Assuming both `mat_head` and `mat_body` are NumPy arrays or matrix-like objects, the resulting matrix `X_mat` will have the same number of rows as the original matrices. The number of columns in `X_mat` will be the sum of the number of columns in `mat_head` and `mat_body`.

The purpose of stacking or concatenating the matrices horizontally is to combine the features or representations derived from different sources (in this case, `mat_head` and `mat_body`). This can be useful in machine learning or data analysis tasks where multiple sets of features need to be considered together for modeling or further analysis.

"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_mat,y,test_size=0.2, random_state=0)

"""The `train_test_split` function is commonly used to split a dataset into training and testing sets for machine learning tasks. It randomly divides the data into two parts based on the specified `test_size` parameter, which determines the proportion of the data to be allocated for testing.

The inputs to the `train_test_split` function are `X_mat` and `y`, representing the feature matrix and target variable, respectively.

The outputs of the function are assigned to four variables: `X_train`, `X_test`, `y_train`, and `y_test`. The `X_train` and `y_train` variables will contain the training data, while `X_test` and `y_test` will hold the testing data.

By specifying `test_size=0.2`, 20% of the data will be allocated for testing, while the remaining 80% will be used for training. The `random_state` parameter is set to 0, which ensures that the random shuffling and splitting of the data will be reproducible.

"""

from sklearn.tree import DecisionTreeClassifier

"""An instance of `DecisionTreeClassifier` named `dtc` is created with the parameter `criterion` set to 'entropy'. The 'entropy' criterion is used to measure the quality of a split in the decision tree based on the information gain.

Next, the `X_train` and `y_train` variables are converted to NumPy arrays using the `np.asarray()` function, and then further converted to one-dimensional arrays using the `np.squeeze()` function. This is done to ensure that the dimensions of the input arrays match the expected format for training the decision tree classifier.

Similarly, the `X_test` array is also converted to a one-dimensional array using `np.squeeze()`.

Finally, the `fit()` method of the `dtc` object is called with `X_train` and `y_train` as the training data. This trains the decision tree classifier on the provided training dataset.

"""

dtc = DecisionTreeClassifier(criterion='entropy')

"""By choosing the 'entropy' criterion for the decision tree classifier, as in the code snippet you provided, the algorithm will use entropy as the measure of impurity to guide the construction of the decision tree. It will aim to make splits that maximize the information gain, reducing the entropy and improving the purity of the resulting subsets at each step of the tree-building process.

"""

X_train = np.squeeze(np.asarray(X_train))
y_train = np.squeeze(np.asarray(y_train))
X_test = np.squeeze(np.asarray(X_test))
dtc.fit(X_train, y_train)

y_pred = dtc.predict(X_test)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

"""The `confusion_matrix` function is commonly used in **classification tasks** to evaluate the performance of a machine learning model by comparing the predicted labels (`y_pred`) with the true labels (`y_test`).

The function takes two arguments: `y_test` and `y_pred`. `y_test` represents the true labels of the test data, while `y_pred` represents the predicted labels generated by the trained model.

When executed, the `confusion_matrix` function will compute a confusion matrix based on the provided labels. The confusion matrix is a table that summarizes the performance of a classification model, showing the counts of true positive, true negative, false positive, and false negative predictions.

The output of the `confusion_matrix` function will be a 2-dimensional array representing the confusion matrix. The rows of the matrix correspond to the true labels, and the columns correspond to the predicted labels. Each element of the matrix represents the count of data points falling into a particular combination of true and predicted labels.

By examining the confusion matrix, you can gain insights into the model's performance, including the accuracy, precision, recall, and other evaluation metrics derived from the counts in the matrix.

"""

print(cm)